{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9edafd4604371e5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# AI-Enabled Choreography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec22b26764a18",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Part 1: Visualize the data\n",
    "PyVista is a helper library for the Visualization Toolkit (VTK) that provides a Pythonic interface for rapid prototyping, analysis, and visualization of spatially referenced datasets. \n",
    "\n",
    "Using Pyvista, we visualized the MarielDataset in a more interactive way.\n",
    "\n",
    "What we can do:\n",
    "\n",
    "* visualize the stick figure\n",
    "* compare the original and ghosted stick figure\n",
    "* construct a slider widget to control the frame\n",
    "* transform the animation into a gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed67996e543519",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T01:28:07.675460Z",
     "start_time": "2024-03-24T01:28:04.350556Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from functions.load_data import *\n",
    "from functions.plotting import animate_stick\n",
    "import pyvista as pv\n",
    "pv.set_jupyter_backend('trame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T01:28:08.892475Z",
     "start_time": "2024-03-24T01:28:07.676536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original numpy dataset contains 38,309 timesteps of 53 joints with 3 dimensions each.\n",
      "loc_min: -1.8967371874141707 loc_max: 1.5558704656286815\n",
      "vel_min: -45.57506836403084 vel_max: 33.951220235113276\n",
      "loc_min: -0.4843721412027978 loc_max: 0.9283637015363149\n",
      "vel_min: -45.57506836403084 vel_max: 33.951220235113276\n",
      "Seeding with frame 28188\n",
      "Seeding with frame 5272\n",
      "(50, 53, 3) (50, 53, 3)\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "ds_all, ds_all_centered, datasets, datasets_centered, ds_counts = load_data(pattern=\"../data/mariel_*.npy\")\n",
    "seq_len = 50\n",
    "index_start = 0\n",
    "index_start = np.random.randint(0,len(ds_all_centered)-seq_len)\n",
    "print(\"Seeding with frame {}\".format(index_start))\n",
    "xtest = ds_all_centered[index_start:index_start + seq_len, :, :3]\n",
    "index_ghost = np.random.randint(0, len(ds_all_centered) - seq_len)\n",
    "print(\"Seeding with frame {}\".format(index_ghost))\n",
    "xtest_ghost = ds_all_centered[index_ghost:index_ghost + seq_len, :, :3]\n",
    "print(xtest.shape, xtest_ghost.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa954e712616f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T01:28:14.372857Z",
     "start_time": "2024-03-24T01:28:08.894475Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede7fb47a7e64b57a31f6ea33da655bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58614/index.html?ui=P_0x237d1155f40_0&reconnect=auto\" class=\"pyvis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# animate the stick figure\n",
    "animate_stick(xtest, output_gif_path=\"../data/animations/test.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba60874262b51690",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "![stick figure](../data/animations/test.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1498c39b8b37fd7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T01:28:18.187720Z",
     "start_time": "2024-03-24T01:28:14.374158Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f38f2e540746f6b64a92fe3a234ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58614/index.html?ui=P_0x2378e329130_1&reconnect=auto\" class=\"pyvis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare the original and ghosted stick figure\n",
    "animate_stick(xtest, xtest_ghost, ghost_shift=0.5, output_gif_path=\"../data/animations/test-ghost.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b846fabfd5b151",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "![stick figure](../data/animations/test-ghost.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee39618f26c11a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Part 2: Train a generative model\n",
    "\n",
    "We chose a Variational Autoencoder (VAE) structure with LSTM layers operating on a fixed size input vector as recommended. However, the architecture of Transformers is also worth trying.\n",
    "\n",
    "![vae-lstm](../images/LSTM-VAE-architecture.png)\n",
    "The VAE component is responsible for learning a latent representation of the input dance sequences. It consists of an encoder network that maps the input dance sequence to a lower-dimensional latent space, and a decoder network that reconstructs the original dance sequence from the latent space. The encoder network captures the essential features and patterns of the input sequences, while the decoder network generates new dance sequences based on the learned latent representation.\n",
    "\n",
    "The LSTM component is a type of recurrent neural network (RNN) that can model the temporal dependencies in the dance sequences. It processes the latent representation generated by the VAE encoder and learns to generate dance steps one at a time, conditioned on the previous steps. By incorporating LSTM, the model can capture the sequential nature of dance movements and generate coherent and realistic dance sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faaf61de2d9352b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T01:28:19.497217Z",
     "start_time": "2024-03-24T01:28:18.190074Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original numpy dataset contains 38,309 timesteps of 53 joints with 3 dimensions each.\n",
      "loc_min: -1.8967371874141707 loc_max: 1.5558704656286815\n",
      "vel_min: -45.57506836403084 vel_max: 33.951220235113276\n",
      "loc_min: -0.4843721412027978 loc_max: 0.9283637015363149\n",
      "vel_min: -45.57506836403084 vel_max: 33.951220235113276\n",
      "\n",
      "Generating overlapping sequences...\n",
      "Using (x,y)-centering...\n",
      "Using all joints...\n"
     ]
    }
   ],
   "source": [
    "from functions.main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef4052ef94552b7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T01:28:19.513226Z",
     "start_time": "2024-03-24T01:28:19.498217Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# training(optional)\n",
    "# run_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda450e69d2e366",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "![loss](../images/train-loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d6639f1161e3a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T01:28:25.510691Z",
     "start_time": "2024-03-24T01:28:19.514505Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf289659cd784755aeebfd4a142434a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58614/index.html?ui=P_0x23798f54ee0_2&reconnect=auto\" class=\"pyvis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# or using the pretrained weights to predict the next frame\n",
    "predict_frame = run_predict(1)\n",
    "predict_frame = predict_frame[0].reshape(1, 53, 6).cpu().detach().numpy()[..., :3]\n",
    "animate_stick(predict_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc580a0aa15b28e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T01:28:40.034789Z",
     "start_time": "2024-03-24T01:28:25.511937Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9493b0f0c7044e23a350c8a8c472069b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:58614/index.html?ui=P_0x23798f3f760_3&reconnect=auto\" class=\"pyvis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using a loop, we can generate dance sequence of any length\n",
    "predict = run_generate(1, 50)\n",
    "predict = predict[0].reshape(predict.size(1), 53, 6).cpu().detach().numpy()[..., :3]\n",
    "animate_stick(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8181a5c9aa0262a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Part 3: Why this project? \n",
    "I am extremely interested in this project that combines art, dance, and artificial intelligence. \n",
    "\n",
    "I have been involved in the intersection of computer science and art for several years. I am now Master of Software Engineering at Nanjing University and got a minor in Culture and Creativity during my undergraduate study. I have worked on several AI+art projects in the past. For example, I constructed diffusion models to accomplish Text2Text, Img2Img, and Inpainting tasks for generating photos with distinctive Chinese traditional characteristics, such as shadow puppetry and New Year painting styles. I have also worked on a project called <a href=\"https://github.com/ChenZiHong-Gavin/chinese-old-movie\">TALES FROM THE THOUSAND AND ONE NIGHTS</a>.  It involved a comprehensive exploration of emotional elements in Chinese classical films, incorporating sentiment analysis and audio-visual processing. It encompassed aspects such as film restoration and colorization.\n",
    "\n",
    "While the VAE+LSTM architecture used in this experiment has shown promising results, there are other architectures, particularly the Transformers, that are worth exploring for this task. It can utilize self-attention mechanisms to attend to different parts of the input sequence, enabling it to learn contextual representations effectively.\n",
    "\n",
    "AI in art opens up a realm of possibilities for artistic exploration, innovation, and engagement. I believe this project is a perfect opportunity to combine my technical skills with my artistic interests and contribute to the field of AI-enabled choreography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6222e853b23412f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T01:28:40.050316Z",
     "start_time": "2024-03-24T01:28:40.035789Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
